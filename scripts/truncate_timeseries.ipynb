{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bc9ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading metadata tables (PATIENTS, ADMISSIONS, ICUSTAYS)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3341666/2714184389.py:95: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  if nan_age_mask.any(): stays['AGE'].fillna(91.4, inplace=True) # Impute missing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial unique counts: ICUSTAY_IDs=61532, HADM_IDs=57786, SUBJECT_IDs=46476\n",
      "Filtering stays: Removing transfers...\n",
      " Stays after removing transfers: 55830 (removed 5702)\n",
      "Merging stays with admissions and patients...\n",
      " Stays after merging: 55830\n",
      "Filtering stays: Keeping only admissions with exactly 1 ICU stay...\n",
      " Stays after keeping HADM_IDs with 1 ICUSTAY: 50186\n",
      "Filtering stays: Calculating and filtering by age (>=18)...\n",
      "OverflowError during age calculation: Overflow in int64 addition.\n",
      " Stays after age filter (>=18): 50186 (removed 0)\n",
      "Adding in-hospital mortality info...\n",
      " Calculated in-hospital mortality for 50186 final valid stays.\n",
      "Saving filtered stays per subject...\n",
      "Total unique subjects with stays after all filters: 41587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving stays.csv: 100%|██████████| 41587/41587 [09:01<00:00, 76.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading event tables IN FULL and processing events per subject...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Event Tables:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Reading full LABEVENTS...\n",
      "  Read 27854055 rows from LABEVENTS.\n",
      "  Kept 23656871 rows after filtering for eligible subjects.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Event Tables: 100%|██████████| 1/1 [02:54<00:00, 174.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading event tables. Processed 23656871 potential events.\n",
      "Saving events.csv for 41587 subjects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   1%|          | 380/41587 [00:11<16:07, 42.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 11152 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   1%|          | 420/41587 [00:12<18:11, 37.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 7411 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   1%|          | 439/41587 [00:13<15:54, 43.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 11284 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   1%|          | 445/41587 [00:13<15:48, 43.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 20429 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   2%|▏         | 843/41587 [00:30<17:07, 39.65it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 59766 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   3%|▎         | 1063/41587 [00:36<20:13, 33.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 4182 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   3%|▎         | 1182/41587 [00:41<30:44, 21.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 8241 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   3%|▎         | 1261/41587 [00:43<18:25, 36.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 23493 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   3%|▎         | 1282/41587 [00:43<17:40, 38.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 4219 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   3%|▎         | 1375/41587 [00:48<24:03, 27.85it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 8277 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   4%|▍         | 1619/41587 [00:57<16:33, 40.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 18157 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   4%|▍         | 1765/41587 [01:03<20:14, 32.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 20810 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   5%|▌         | 2143/41587 [01:15<17:35, 37.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 10522 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   6%|▌         | 2302/41587 [01:20<19:49, 33.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 79364 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   6%|▌         | 2485/41587 [01:27<24:35, 26.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 19318 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   6%|▌         | 2538/41587 [01:29<21:52, 29.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 23856 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   7%|▋         | 2713/41587 [01:34<21:39, 29.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 22313 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   7%|▋         | 2800/41587 [01:37<17:36, 36.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 20875 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   7%|▋         | 2863/41587 [01:39<18:05, 35.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 7944 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   8%|▊         | 3139/41587 [01:48<18:18, 34.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 4341 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   8%|▊         | 3420/41587 [01:57<20:22, 31.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 17674 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   9%|▊         | 3567/41587 [02:02<20:08, 31.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 8320 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:   9%|▊         | 3636/41587 [02:04<15:52, 39.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 4501 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:  10%|▉         | 4072/41587 [02:19<15:36, 40.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 21245 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:  10%|▉         | 4103/41587 [02:20<15:34, 40.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 6679 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:  10%|█         | 4192/41587 [02:23<26:51, 23.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 15721 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:  10%|█         | 4223/41587 [02:24<17:17, 36.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 21404 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:  10%|█         | 4241/41587 [02:24<17:25, 35.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 14718 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:  11%|█         | 4428/41587 [02:30<18:00, 34.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 13437 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:  11%|█         | 4438/41587 [02:31<18:40, 33.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 531 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:  11%|█         | 4447/41587 [02:31<18:05, 34.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 15962 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:  12%|█▏        | 4788/41587 [02:41<19:16, 31.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for subject 5267 after processing all tables. events.csv NOT created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving events.csv:  12%|█▏        | 4812/41587 [02:42<20:43, 29.56it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 244\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# event_tables = ['CHARTEVENTS', 'LABEVENTS', 'OUTPUTEVENTS']\u001b[39;00m\n\u001b[32m    241\u001b[39m event_tables = [\u001b[33m'\u001b[39m\u001b[33mLABEVENTS\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m \u001b[43mextract_subject_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmimic3_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mevent_tables\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Pass debug ID to function\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 227\u001b[39m, in \u001b[36mextract_subject_data\u001b[39m\u001b[34m(mimic3_path, output_path, event_tables)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;66;03m# Convert CHARTTIME to string before saving\u001b[39;00m\n\u001b[32m    226\u001b[39m     final_df[\u001b[33m'\u001b[39m\u001b[33mCHARTTIME\u001b[39m\u001b[33m'\u001b[39m] = final_df[\u001b[33m'\u001b[39m\u001b[33mCHARTTIME\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[43mfinal_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mQUOTE_MINIMAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# print(f\"Saved events.csv for subject {subject_id_str} ({len(final_df)} events).\")\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mimic3_denis_project/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mimic3_denis_project/lib/python3.11/site-packages/pandas/core/generic.py:3967\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3956\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3958\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3959\u001b[39m     frame=df,\n\u001b[32m   3960\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3964\u001b[39m     decimal=decimal,\n\u001b[32m   3965\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3967\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3970\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3972\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3984\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mimic3_denis_project/lib/python3.11/site-packages/pandas/io/formats/format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mimic3_denis_project/lib/python3.11/site-packages/pandas/io/formats/csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mimic3_denis_project/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m    876\u001b[39m             encoding=ioargs.encoding,\n\u001b[32m    877\u001b[39m             errors=errors,\n\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:186\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, errors)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm # Use notebook version of tqdm\n",
    "import sys # To stream logs to notebook output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Cell 3: Main Extraction Logic (Modified for ALL subjects, NO event chunking)\n",
    "def extract_subject_data(mimic3_path, output_path, event_tables):\n",
    "    \"\"\"Extracts per-subject stays and events data for ALL eligible subjects (Loads full event tables).\"\"\"\n",
    "\n",
    "\n",
    "    # --- Read and Prepare Metadata Tables ---\n",
    "    print('Reading metadata tables (PATIENTS, ADMISSIONS, ICUSTAYS)...')\n",
    "    try:\n",
    "        # (Keep the robust reading logic from before)\n",
    "        pats = pd.read_csv(\n",
    "            os.path.join(mimic3_path, 'PATIENTS.csv'),\n",
    "            header=0, index_col=None, dtype={'SUBJECT_ID': int},\n",
    "            usecols=['SUBJECT_ID', 'GENDER', 'DOB', 'DOD'])\n",
    "        pats['DOB'] = pd.to_datetime(pats['DOB'], errors='coerce')\n",
    "        pats['DOD'] = pd.to_datetime(pats['DOD'], errors='coerce')\n",
    "\n",
    "        admits = pd.read_csv(\n",
    "            os.path.join(mimic3_path, 'ADMISSIONS.csv'),\n",
    "            header=0, index_col=None, dtype={'SUBJECT_ID': int, 'HADM_ID': int},\n",
    "            usecols=['SUBJECT_ID', 'HADM_ID', 'ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'ADMISSION_TYPE', 'ETHNICITY', 'DIAGNOSIS'])\n",
    "        admits['ADMITTIME'] = pd.to_datetime(admits['ADMITTIME'], errors='coerce')\n",
    "        admits['DISCHTIME'] = pd.to_datetime(admits['DISCHTIME'], errors='coerce')\n",
    "        admits['DEATHTIME'] = pd.to_datetime(admits['DEATHTIME'], errors='coerce')\n",
    "\n",
    "        stays = pd.read_csv(\n",
    "            os.path.join(mimic3_path, 'ICUSTAYS.csv'),\n",
    "            header=0, index_col=None, dtype={'SUBJECT_ID': int, 'HADM_ID': int, 'ICUSTAY_ID': int})\n",
    "        required_stay_cols = {'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'INTIME', 'OUTTIME',\n",
    "                              'FIRST_WARDID', 'LAST_WARDID', 'FIRST_CAREUNIT', 'LAST_CAREUNIT', 'LOS'}\n",
    "        if not required_stay_cols.issubset(stays.columns):\n",
    "            missing = required_stay_cols - set(stays.columns)\n",
    "            print(f\"Missing required columns in ICUSTAYS.csv: {missing}\")\n",
    "            return\n",
    "        stays['INTIME'] = pd.to_datetime(stays['INTIME'], errors='coerce')\n",
    "        stays['OUTTIME'] = pd.to_datetime(stays['OUTTIME'], errors='coerce')\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error reading input CSV file: {e}. Please check mimic3_path.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error during table reading or initial date conversion: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    print(f\"Initial unique counts: ICUSTAY_IDs={stays['ICUSTAY_ID'].nunique()}, HADM_IDs={stays['HADM_ID'].nunique()}, SUBJECT_IDs={stays['SUBJECT_ID'].nunique()}\")\n",
    "\n",
    "    # --- Filter Stays (Applied to ALL subjects) ---\n",
    "    # (Keep the same filtering logic as before: transfers, merge, 1 ICU stay, age, mortality)\n",
    "    print('Filtering stays: Removing transfers...')\n",
    "    original_rows = stays.shape[0]\n",
    "    stays = stays.loc[(stays['FIRST_WARDID'] == stays['LAST_WARDID']) & (stays['FIRST_CAREUNIT'] == stays['LAST_CAREUNIT'])]\n",
    "    stays = stays.drop(columns=['FIRST_WARDID', 'LAST_WARDID', 'FIRST_CAREUNIT'], errors='ignore')\n",
    "    print(f\" Stays after removing transfers: {stays.shape[0]} (removed {original_rows - stays.shape[0]})\")\n",
    "\n",
    "    print(\"Merging stays with admissions and patients...\")\n",
    "    try:\n",
    "        stays = stays.merge(admits, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "        stays = stays.merge(pats, on=['SUBJECT_ID'], how='inner')\n",
    "        print(f\" Stays after merging: {stays.shape[0]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during merging of tables: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    print('Filtering stays: Keeping only admissions with exactly 1 ICU stay...')\n",
    "    icu_counts = stays.groupby('HADM_ID')['ICUSTAY_ID'].transform('count')\n",
    "    stays = stays[icu_counts == 1].copy()\n",
    "    print(f\" Stays after keeping HADM_IDs with 1 ICUSTAY: {stays.shape[0]}\")\n",
    "\n",
    "    print(\"Filtering stays: Calculating and filtering by age (>=18)...\")\n",
    "    valid_dates_mask = stays['INTIME'].notna() & stays['DOB'].notna()\n",
    "    stays['AGE'] = np.nan\n",
    "    if valid_dates_mask.any():\n",
    "        try:\n",
    "            time_diff_valid = stays.loc[valid_dates_mask, 'INTIME'] - stays.loc[valid_dates_mask, 'DOB']\n",
    "            valid_diff_mask = valid_dates_mask & time_diff_valid.notna()\n",
    "            if valid_diff_mask.any():\n",
    "                 age_in_days_valid = time_diff_valid[valid_diff_mask].dt.days\n",
    "                 stays.loc[valid_diff_mask, 'AGE'] = age_in_days_valid / 365.25\n",
    "        except OverflowError as e: print(f\"OverflowError during age calculation: {e}.\")\n",
    "        except Exception as e: print(f\"Unexpected error during age calculation: {e}\", exc_info=True)\n",
    "    age_ge_89_mask = stays['AGE'] < 0\n",
    "    if age_ge_89_mask.any(): stays.loc[age_ge_89_mask, 'AGE'] = 91.4\n",
    "    nan_age_mask = stays['AGE'].isna()\n",
    "    if nan_age_mask.any(): stays['AGE'].fillna(91.4, inplace=True) # Impute missing\n",
    "    original_count_before_age_filter = stays.shape[0]\n",
    "    stays = stays.loc[stays['AGE'] >= 18].copy()\n",
    "    print(f\" Stays after age filter (>=18): {stays.shape[0]} (removed {original_count_before_age_filter - stays.shape[0]})\")\n",
    "\n",
    "    if stays.empty:\n",
    "        print(\"No stays remaining after all filtering. Stopping.\")\n",
    "        return\n",
    "\n",
    "    print('Adding in-hospital mortality info...')\n",
    "    date_cols_for_mort = ['ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'DOD']\n",
    "    for col in date_cols_for_mort:\n",
    "        if col in stays.columns: stays[col] = pd.to_datetime(stays[col], errors='coerce')\n",
    "    died_in_hosp_dod = (stays['DOD'].notna()) & (stays['ADMITTIME'].notna()) & (stays['DISCHTIME'].notna()) & (stays['ADMITTIME'] <= stays['DOD']) & (stays['DISCHTIME'] >= stays['DOD'])\n",
    "    died_in_hosp_deathtime = (stays['DEATHTIME'].notna()) & (stays['ADMITTIME'].notna()) & (stays['DISCHTIME'].notna()) & (stays['ADMITTIME'] <= stays['DEATHTIME']) & (stays['DISCHTIME'] >= stays['DEATHTIME'])\n",
    "    stays['MORTALITY'] = (died_in_hosp_dod | died_in_hosp_deathtime).astype(int)\n",
    "    print(f\" Calculated in-hospital mortality for {len(stays)} final valid stays.\")\n",
    "\n",
    "\n",
    "    # --- Save stays.csv per subject (for ALL eligible subjects) ---\n",
    "    print('Saving filtered stays per subject...')\n",
    "    subjects_with_stays_ids = stays['SUBJECT_ID'].unique()\n",
    "    print(f\"Total unique subjects with stays after all filters: {len(subjects_with_stays_ids)}\")\n",
    "\n",
    "    subjects_to_process_events_for = set(str(s) for s in subjects_with_stays_ids)\n",
    "\n",
    "    for subject_id_int in tqdm(subjects_with_stays_ids, desc=\"Saving stays.csv\"):\n",
    "        subject_id_str = str(subject_id_int)\n",
    "        subject_stays = stays.loc[stays['SUBJECT_ID'] == subject_id_int].sort_values(by='INTIME')\n",
    "        dn = os.path.join(output_path, subject_id_str)\n",
    "        os.makedirs(dn, exist_ok=True)\n",
    "        try:\n",
    "            for col in subject_stays.select_dtypes(include=['datetime64[ns]']).columns:\n",
    "                 subject_stays[col] = subject_stays[col].astype(str)\n",
    "            subject_stays.to_csv(os.path.join(dn, 'stays.csv'), index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving stays.csv for subject {subject_id_str}: {e}\")\n",
    "\n",
    "\n",
    "    # --- Process and Save events.csv per subject (LOADING FULL TABLES) ---\n",
    "    print('Reading event tables IN FULL and processing events per subject...')\n",
    "    event_columns = {\n",
    "        'CHARTEVENTS': ['SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'CHARTTIME', 'ITEMID', 'VALUE', 'VALUEUOM'],\n",
    "        'LABEVENTS': ['SUBJECT_ID', 'HADM_ID', 'CHARTTIME', 'ITEMID', 'VALUE', 'VALUEUOM'],\n",
    "        'OUTPUTEVENTS': ['SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'CHARTTIME', 'ITEMID', 'VALUE', 'VALUEUOM']\n",
    "    }\n",
    "    event_dtypes = {\n",
    "        'SUBJECT_ID': 'int32', 'HADM_ID': 'float64', 'ICUSTAY_ID': 'float64',\n",
    "        'ITEMID': 'int32', 'VALUE': 'object', 'VALUEUOM': 'object'\n",
    "    }\n",
    "    obs_header = ['SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'CHARTTIME', 'ITEMID', 'VALUE', 'VALUEUOM']\n",
    "\n",
    "    # Dictionary to hold list of event dataframes for each subject\n",
    "    subject_events_data = {subj_id: [] for subj_id in subjects_to_process_events_for}\n",
    "    total_events_processed = 0\n",
    "\n",
    "    for table in tqdm(event_tables, desc=\"Processing Event Tables\"):\n",
    "        print(f\" Reading full {table}...\")\n",
    "        tn = os.path.join(mimic3_path, table + '.csv')\n",
    "        if not os.path.exists(tn):\n",
    "            print(f\" {table}.csv not found in {mimic3_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Read the entire table - REQUIRES SIGNIFICANT MEMORY\n",
    "            df_table = pd.read_csv(\n",
    "                tn,\n",
    "                usecols=event_columns[table],\n",
    "                dtype=event_dtypes,\n",
    "                parse_dates=['CHARTTIME'],\n",
    "                low_memory=False\n",
    "            )\n",
    "            print(f\"  Read {len(df_table)} rows from {table}.\")\n",
    "\n",
    "            # Filter to only eligible subjects (convert table subject_id to string for matching)\n",
    "            df_table['SUBJECT_ID'] = df_table['SUBJECT_ID'].astype(str)\n",
    "            df_filtered = df_table[df_table['SUBJECT_ID'].isin(subjects_to_process_events_for)].copy() # Filter\n",
    "            print(f\"  Kept {len(df_filtered)} rows after filtering for eligible subjects.\")\n",
    "            del df_table # Free memory\n",
    "\n",
    "            if df_filtered.empty: continue # Skip if no events for eligible subjects in this table\n",
    "\n",
    "            # Data Cleaning\n",
    "            df_filtered['HADM_ID'] = df_filtered['HADM_ID'].fillna(-1).astype(int)\n",
    "            if 'ICUSTAY_ID' in df_filtered.columns:\n",
    "                df_filtered['ICUSTAY_ID'] = df_filtered['ICUSTAY_ID'].fillna(-1).astype(int)\n",
    "            else:\n",
    "                df_filtered['ICUSTAY_ID'] = -1\n",
    "            # Convert IDs back to string for consistency\n",
    "            df_filtered['SUBJECT_ID'] = df_filtered['SUBJECT_ID'].astype(str)\n",
    "            df_filtered['HADM_ID'] = df_filtered['HADM_ID'].astype(str)\n",
    "            df_filtered['ICUSTAY_ID'] = df_filtered['ICUSTAY_ID'].astype(str)\n",
    "            df_filtered['ITEMID'] = df_filtered['ITEMID'].astype(str)\n",
    "            df_filtered['VALUEUOM'] = df_filtered['VALUEUOM'].fillna('').astype(str)\n",
    "            df_filtered['VALUE'] = df_filtered['VALUE'].astype(str)\n",
    "\n",
    "            original_len = len(df_filtered)\n",
    "            df_filtered['CHARTTIME'] = pd.to_datetime(df_filtered['CHARTTIME'], errors='coerce')\n",
    "            df_filtered.dropna(subset=['CHARTTIME'], inplace=True)\n",
    "            # if len(df_filtered) < original_len:\n",
    "            #     print(f\" Dropped {original_len - len(df_filtered)} rows from {table} due to invalid CHARTTIME.\")\n",
    "\n",
    "            df_filtered = df_filtered[obs_header] # Reorder/select columns\n",
    "\n",
    "            # Append processed data to the dictionary keyed by subject_id\n",
    "            for subject_id_str, group in df_filtered.groupby('SUBJECT_ID'):\n",
    "                subject_events_data[subject_id_str].append(group)\n",
    "                total_events_processed += len(group) # Track total events\n",
    "\n",
    "        except MemoryError:\n",
    "             print(f\"MEMORY ERROR while processing {table}! Cannot load full table. Please use chunking.\")\n",
    "             # Optionally re-raise or return to stop execution\n",
    "             raise\n",
    "        except Exception as e:\n",
    "             print(f\"Error processing {table}: {e}\", exc_info=True)\n",
    "             # Continue to next table if possible\n",
    "\n",
    "    # --- Concatenate and Save Events for each subject ---\n",
    "    print(f\"Finished reading event tables. Processed {total_events_processed} potential events.\")\n",
    "    print(f\"Saving events.csv for {len(subjects_to_process_events_for)} subjects...\")\n",
    "\n",
    "    for subject_id_str in tqdm(subjects_to_process_events_for, desc=\"Saving events.csv\"):\n",
    "        dn = os.path.join(output_path, subject_id_str)\n",
    "        fn = os.path.join(dn, 'events.csv')\n",
    "        list_of_dfs = subject_events_data.get(subject_id_str, []) # Get list for subject\n",
    "\n",
    "        if list_of_dfs: # Check if any events were collected for this subject\n",
    "            try:\n",
    "                final_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "                final_df.sort_values(by='CHARTTIME', inplace=True)\n",
    "                # Convert CHARTTIME to string before saving\n",
    "                final_df['CHARTTIME'] = final_df['CHARTTIME'].astype(str)\n",
    "                final_df.to_csv(fn, index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "                # print(f\"Saved events.csv for subject {subject_id_str} ({len(final_df)} events).\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during concat/sort/save of events.csv for {subject_id_str}: {e}\", exc_info=True)\n",
    "        else:\n",
    "            print(f\"No events found for subject {subject_id_str} after processing all tables. events.csv NOT created.\")\n",
    "\n",
    "\n",
    "# Cell 4: Run Extraction within Jupyter\n",
    "if __name__ == \"__main__\" : # Detect if running in Jupyter/IPython\n",
    "\n",
    "    mimic3_path = \"/sise/robertmo-group/Eldar/projects/mimic_preprocessing/MIMIC3_data/mimic-1.4\"\n",
    "    output_path = \"/sise/robertmo-group/Eldar/projects/mortality_prediction_denis_project/test_debug2\"\n",
    "    event_tables = ['CHARTEVENTS', 'LABEVENTS', 'OUTPUTEVENTS']\n",
    "\n",
    "\n",
    "\n",
    "    extract_subject_data(mimic3_path,output_path,event_tables) # Pass debug ID to function\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
